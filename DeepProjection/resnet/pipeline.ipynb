{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "import torch.nn.functional as F\r\n",
    "import torchvision.models as models\r\n",
    "import torchvision\r\n",
    "import torchvision.transforms as transforms\r\n",
    "from torch.utils.data import Subset, Dataset, DataLoader\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import h5py\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from PIL import Image\r\n",
    "import argparse\r\n",
    "from argparse import Namespace\r\n",
    "\r\n",
    "import os\r\n",
    "import random\r\n",
    "from tqdm import tqdm\r\n",
    "\r\n",
    "import tensorboard\r\n",
    "from torchsummary import summary\r\n",
    "from torch.utils.tensorboard import SummaryWriter\r\n",
    "\r\n",
    "from resnet import resnet18, resnet34, resnet50\r\n",
    "from datetime import datetime as dt\r\n",
    "\r\n",
    "if torch.cuda.is_available():\r\n",
    "    device = torch.device(\"cuda\")\r\n",
    "else:\r\n",
    "    device = torch.device(\"cpu\")"
   ],
   "outputs": [],
   "metadata": {
    "executionInfo": {
     "elapsed": 2076,
     "status": "ok",
     "timestamp": 1621182233169,
     "user": {
      "displayName": "Enci Liu",
      "photoUrl": "",
      "userId": "13274077478020183171"
     },
     "user_tz": 420
    },
    "id": "ocNRQsIznK8n"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "device"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "id": "I7gi1BCstuFv"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "particle2idx = {\r\n",
    "    '1fpv': 0,\r\n",
    "    '1ss8': 1,\r\n",
    "    '3j03': 2,\r\n",
    "    '1ijg': 3,\r\n",
    "    '3iyf': 4,\r\n",
    "    '6ody': 5,\r\n",
    "    '6sp2': 6,\r\n",
    "    '6xs6': 7,\r\n",
    "    '7dwz': 8,\r\n",
    "    '7dx8': 9,\r\n",
    "    '7dx9': 10\r\n",
    "}\r\n",
    "\r\n",
    "count2idx = {\r\n",
    "    'single': 0,\r\n",
    "    'double': 1,\r\n",
    "    'triple': 2,\r\n",
    "    'quadruple': 3\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "idx2particle = {\r\n",
    "    0: '1fpv',\r\n",
    "    2: '1ss8',\r\n",
    "    2: '3j03',\r\n",
    "    3: '1ijg',\r\n",
    "    4: '3iyf',\r\n",
    "    5: '6ody',\r\n",
    "    6: '6sp2',\r\n",
    "    7: '6xs6',\r\n",
    "    8: '7dwz',\r\n",
    "    9: '7dx8',\r\n",
    "    10: '7dx9'\r\n",
    "}\r\n",
    "\r\n",
    "idx2count = {\r\n",
    "    0: 'single',\r\n",
    "    1: 'double',\r\n",
    "    2: 'triple',\r\n",
    "    3: 'quadruple'\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\r\n",
    "    def __init__(self, root_dir, particles, counts, transform=None, seed=1234):\r\n",
    "        self.root_dir = root_dir\r\n",
    "        self.transform = transform\r\n",
    "\r\n",
    "        self.count_labels = []\r\n",
    "        self.particle_labels = []\r\n",
    "        self.data = []\r\n",
    "\r\n",
    "        for particle in particles:\r\n",
    "            for count in counts:\r\n",
    "                \r\n",
    "                # Create directory path to dataset\r\n",
    "                n = 1\r\n",
    "                data_dir = f'{self.root_dir}/{particle}_{str(n)}k_{count}_pps_1e14_thumbnail.h5'\r\n",
    "\r\n",
    "                # Load images as h5 files\r\n",
    "                f = h5py.File(data_dir, 'r')\r\n",
    "                dset_name = list(f.keys())[0]\r\n",
    "                data = f[dset_name]\r\n",
    "                data = [Image.fromarray(data[i]) for i in range(LENGTH * n)]\r\n",
    "                data = [self.transform(data[i]) for i in range(LENGTH * n)]\r\n",
    "                count_label = [count2idx[count]] * (LENGTH * n)\r\n",
    "                particle_label = [particle2idx[particle]] * (LENGTH * n)\r\n",
    "                self.data.extend(data)\r\n",
    "                self.count_labels.extend(count_label)\r\n",
    "                self.particle_labels.extend(particle_label)\r\n",
    "        \r\n",
    "        # Shuffle the data\r\n",
    "        random.seed(seed)\r\n",
    "        perm = list(range(len(self.data)))\r\n",
    "        random.shuffle(perm)\r\n",
    "        self.data = [self.data[i] for i in perm]\r\n",
    "        self.count_labels = [self.count_labels[i] for i in perm]\r\n",
    "        self.particle_labels = [self.particle_labels[i] for i in perm]\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        '''Denotes the total number of samples'''\r\n",
    "        return len(self.data)\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        '''Generates one sample of data'''\r\n",
    "        X = self.data[index]\r\n",
    "        count = self.count_labels[index]\r\n",
    "        particle = self.particle_labels[index]\r\n",
    "        return X, count, particle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataloader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "class AddNoise(object):\r\n",
    "    \"\"\"\r\n",
    "    A torchvision.transforms wrapper for addNoise()\r\n",
    "    \r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, flux_jitter, gaussian_noise):\r\n",
    "        assert isinstance(flux_jitter, float)\r\n",
    "        assert isinstance(gaussian_noise, float)\r\n",
    "        self.flux_jitter = flux_jitter\r\n",
    "        self.gaussian_noise = gaussian_noise\r\n",
    "\r\n",
    "    def __call__(self, image):\r\n",
    "        return self.addNoise(image)\r\n",
    "\r\n",
    "    def addNoise(self, orig_img):\r\n",
    "\r\n",
    "        def changeIntensity(img, flux_jitter):\r\n",
    "            factor = 100 # FIXME: correct for data which has 100 more flux\r\n",
    "            mu = 1 # mean jitter\r\n",
    "            alpha = np.random.normal(mu, flux_jitter)\r\n",
    "            if alpha <= 0: alpha = 0.1 # alpha can't be zero\r\n",
    "            n_photons   = alpha*np.sum(img)/factor                     # number of desired photons per image\r\n",
    "            return n_photons*(img/np.sum(img)) # cache noise-free measurement\r\n",
    "    \r\n",
    "        def poisson(img):\r\n",
    "            # add poisson noise\r\n",
    "            return np.random.poisson(img)      # apply Poisson statistics\r\n",
    "    \r\n",
    "        def gaussian(img, sigma):\r\n",
    "            # add gaussian noise \r\n",
    "            # For random samples from N(\\mu, \\sigma^2), \r\n",
    "            # mu + sigma * np.random.randn(...)\r\n",
    "            # sigma: Gaussian noise level\r\n",
    "            img = img + sigma*np.random.randn(*img.shape);  # apply Gaussian statistics\r\n",
    "            return img\r\n",
    "    \r\n",
    "        def varNorm(V):\r\n",
    "            # variance normalization, each image has mean 0, variance 1\r\n",
    "            # This shouldn't happen, but zero out infinite pixels\r\n",
    "            V[np.argwhere(V==np.inf)] = 0\r\n",
    "            mean = np.mean(V)\r\n",
    "            std = np.std(V)\r\n",
    "            if std == 0:\r\n",
    "                return np.zeros_like(V)\r\n",
    "            V1 = (V-mean)/std\r\n",
    "            return V1\r\n",
    "\r\n",
    "        def transform(img):\r\n",
    "            img = changeIntensity(img, self.flux_jitter)\r\n",
    "            img = poisson(img)\r\n",
    "            img = gaussian(img, self.gaussian_noise)\r\n",
    "            img = varNorm(img)\r\n",
    "            return img\r\n",
    "\r\n",
    "        return transform(orig_img)\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def get_dataloaders(args, train_val_particles, test_particles, test_diff_particle=False):\r\n",
    "    # Original from Shawn\r\n",
    "    #transform = transforms.Compose([transforms.CenterCrop(128),\r\n",
    "    #                                transforms.RandomVerticalFlip(p=0.5),\r\n",
    "    #                                transforms.RandomHorizontalFlip(p=0.5),\r\n",
    "    #                                transforms.ToTensor()])\r\n",
    "    \r\n",
    "    # Modified by EricFlorin\r\n",
    "    transform = transforms.Compose([AddNoise(0.9, 1.0),\r\n",
    "                                    transforms.RandomVerticalFlip(p=0.5),\r\n",
    "                                    transforms.RandomHorizontalFlip(p=0.5),\r\n",
    "                                    transforms.ToTensor()])\r\n",
    "    \r\n",
    "    data_len = args.num_particles * 7000\r\n",
    "    \r\n",
    "    if not test_diff_particle:\r\n",
    "        assert train_val_particles == test_particles\r\n",
    "        dataset = CustomDataset(root_dir=args.root_dir,\r\n",
    "                                particles=train_val_particles,\r\n",
    "                                counts=COUNTS,\r\n",
    "                                transform=transform)\r\n",
    "        print(len(dataset))\r\n",
    "        train_idx = list(range(0, int(data_len * 0.7)))\r\n",
    "        valid_idx = list(range(int(data_len * 0.7), int(data_len * 0.8)))\r\n",
    "        test_idx = list(range(int(data_len * 0.8), data_len))\r\n",
    "        train_dataset = Subset(dataset, train_idx) \r\n",
    "        valid_dataset = Subset(dataset, valid_idx)\r\n",
    "        test_dataset = Subset(dataset, test_idx)\r\n",
    "    else:\r\n",
    "        # Create train/valid/test datasets\r\n",
    "        train_val_dataset = CustomDataset(root_dir=args.root_dir, \r\n",
    "                                          particles=train_val_particles,\r\n",
    "                                          counts=COUNTS,\r\n",
    "                                          transform=transform)\r\n",
    "        train_idx = list(range(0, 7000))\r\n",
    "        valid_idx = list(range(7000, 8000))\r\n",
    "        train_dataset = Subset(train_val_dataset, train_idx) \r\n",
    "        valid_dataset = Subset(train_val_dataset, valid_idx)\r\n",
    "        test_dataset = CustomDataset(root_dir=args.root_dir, \r\n",
    "                                    particles=test_particles,\r\n",
    "                                    counts=COUNTS,\r\n",
    "                                    transform=transform)\r\n",
    "        \r\n",
    "        assert train_dataset.__getitem__(0)[0].shape == torch.Size([1, 128, 128])\r\n",
    "        assert valid_dataset.__getitem__(0)[0].shape == torch.Size([1, 128, 128])\r\n",
    "        assert test_dataset.__getitem__(0)[0].shape == torch.Size([1, 128, 128])\r\n",
    "\r\n",
    "    # Create train/valid/test dataloaders\r\n",
    "    train_dataloader = DataLoader(dataset=train_dataset,\r\n",
    "                                  batch_size=args.batch_size,\r\n",
    "                                  shuffle=args.shuffle, \r\n",
    "                                  num_workers=args.num_workers)\r\n",
    "    valid_dataloader = DataLoader(dataset=valid_dataset,\r\n",
    "                                  batch_size=args.batch_size,\r\n",
    "                                  shuffle=args.shuffle, \r\n",
    "                                  num_workers=args.num_workers)\r\n",
    "    test_dataloader = DataLoader(dataset=test_dataset, \r\n",
    "                                 batch_size=args.batch_size, \r\n",
    "                                 shuffle=args.shuffle, \r\n",
    "                                 num_workers=args.num_workers)\r\n",
    "    return train_dataloader, valid_dataloader, test_dataloader"
   ],
   "outputs": [],
   "metadata": {
    "executionInfo": {
     "elapsed": 394,
     "status": "ok",
     "timestamp": 1621182238687,
     "user": {
      "displayName": "Enci Liu",
      "photoUrl": "",
      "userId": "13274077478020183171"
     },
     "user_tz": 420
    },
    "id": "d_-YQ56vVwhn"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utils"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def sp_noise(image,prob):\r\n",
    "#    '''\r\n",
    "#    Add salt and pepper noise to image\r\n",
    "#    prob: Probability of the noise\r\n",
    "#    '''\r\n",
    "#    output = np.zeros(image.shape,np.uint8)\r\n",
    "#    thres = 1 - prob \r\n",
    "#    for i in range(image.shape[0]):\r\n",
    "#        for j in range(image.shape[1]):\r\n",
    "#            rdn = random.random()\r\n",
    "#            if rdn < prob:\r\n",
    "#                output[i][j] = 0\r\n",
    "#            elif rdn > thres:\r\n",
    "#                output[i][j] = 255\r\n",
    "#            else:\r\n",
    "#                output[i][j] = image[i][j]\r\n",
    "#    return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def read_thumbnails(fname):\r\n",
    "    f = h5py.File(fname, 'r')\r\n",
    "    dset_name = list(f.keys())[0]\r\n",
    "    print(\"Check the datasets keys: \" + str(list(f.keys())))\r\n",
    "    \r\n",
    "    dset = f[dset_name]\r\n",
    "    print(\"Check the shape of the dataset: \" + str(dset.shape))\r\n",
    "    \r\n",
    "    print(len(dset))\r\n",
    "    w=20\r\n",
    "    h=20\r\n",
    "    fig=plt.figure(figsize=(15, 15))\r\n",
    "    columns = 4\r\n",
    "    rows = 5\r\n",
    "    for i in range(1, columns*rows +1):\r\n",
    "        img = dset[5* i]\r\n",
    "        fig.add_subplot(rows, columns, i)\r\n",
    "        plt.imshow(img, vmin=0, vmax=10)\r\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Combine two h5 files and save as new file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "root_dir = './data/thumbnail'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "particle = '1fpv'\r\n",
    "filenames = [f'{root_dir}/SPI_{particle}_1k_single_thumbnail.h5',\r\n",
    "             f'{root_dir}/SPI_{particle}_3k_single_thumbnail.h5']\r\n",
    "\r\n",
    "dataset = []\r\n",
    "\r\n",
    "for fname in filenames:\r\n",
    "    f = h5py.File(fname,'r+')\r\n",
    "    dset_name = list(f.keys())[0]\r\n",
    "    dset = f[dset_name]\r\n",
    "    dataset.extend(dset)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fout = h5py.File(f'{root_dir}/SPI_{particle}_4k_single_thumbnail.h5','w')\r\n",
    "ds = fout.create_dataset('photons', (4000, 128, 130), dtype='float32')\r\n",
    "for i in range(4000):\r\n",
    "    ds[i,:,:] = dataset[i]\r\n",
    "fout.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "particle = '6xs6'\r\n",
    "root_dir = './data/thumbnail'\r\n",
    "image_dir = f'{root_dir}/SPI_{particle}_4k_single_thumbnail.h5'\r\n",
    "\r\n",
    "read_thumbnails(image_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models"
   ],
   "metadata": {
    "id": "bF6OxCUL9r3n"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3-layer Multi-output CNN (Late)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class MultiOutputCNN(nn.Module):\r\n",
    "    def __init__(self, num_particles=11, num_counts=4, hidden_dim=8):\r\n",
    "        super(MultiOutputCNN, self).__init__()\r\n",
    "        self.conv1 = nn.Conv2d(1, hidden_dim, 2, 2) # (8, 64, 64)\r\n",
    "        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim * 4, 4, 4) # (32, 16, 16)\r\n",
    "        self.conv3 = nn.Conv2d(hidden_dim * 4, hidden_dim * 16, 4, 4) # (128, 4, 4)\r\n",
    "        self.dropout1 = nn.Dropout(0.25)\r\n",
    "        self.fc1 = nn.Linear(2048, num_counts)\r\n",
    "        self.fc2 = nn.Linear(2048, num_particles)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.conv1(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv2(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv3(x)\r\n",
    "        x = F.relu(x)\r\n",
    "#         x = self.conv4(x)\r\n",
    "#         x = F.relu(x)\r\n",
    "#         x = self.conv5(x)\r\n",
    "#         x = F.relu(x)\r\n",
    "        x = self.dropout1(x)\r\n",
    "        x = torch.flatten(x, 1)\r\n",
    "\r\n",
    "        y1 = self.fc1(x)\r\n",
    "        y1 = F.log_softmax(y1, dim=1)\r\n",
    "        y2 = self.fc2(x)\r\n",
    "        y2 = F.log_softmax(y2, dim=1)\r\n",
    "        return y1, y2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5-layer Multi-output CNN (Late)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class MultiOutputCNN(nn.Module):\r\n",
    "    def __init__(self, num_particles=11, num_counts=4, hidden_dim=8):\r\n",
    "        super(MultiOutputCNN, self).__init__()\r\n",
    "        self.conv1 = nn.Conv2d(1, hidden_dim, 2, 2) # (8, 64, 64)\r\n",
    "        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim * 2, 2, 2) # (16, 32, 32)\r\n",
    "        self.conv3 = nn.Conv2d(hidden_dim * 2, hidden_dim * 4, 2, 2) # (32, 16, 16)\r\n",
    "        self.conv4 = nn.Conv2d(hidden_dim * 4, hidden_dim * 8, 2, 2) # (64, 8, 8)\r\n",
    "        self.conv5 = nn.Conv2d(hidden_dim * 8, hidden_dim * 16, 2, 2) # (128, 4, 4)\r\n",
    "        self.dropout1 = nn.Dropout(0.25)\r\n",
    "        self.dropout2 = nn.Dropout(0.5)\r\n",
    "        self.fc1 = nn.Linear(2048, num_counts)\r\n",
    "        self.fc2 = nn.Linear(2048, num_particles)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.conv1(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv2(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv3(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv4(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv5(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.dropout1(x)\r\n",
    "        x = torch.flatten(x, 1)\r\n",
    "\r\n",
    "        y1 = self.fc1(x)\r\n",
    "        y1 = F.log_softmax(y1, dim=1)\r\n",
    "        y2 = self.fc2(x)\r\n",
    "        y2 = F.log_softmax(y2, dim=1)\r\n",
    "        return y1, y2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 10-layer Multi-output CNN (Late)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class MultiOutputCNN(nn.Module):\r\n",
    "    def __init__(self, num_particles=11, num_counts=4, hidden_dim=8):\r\n",
    "        super(MultiOutputCNN, self).__init__()\r\n",
    "        self.conv1 = nn.Conv2d(1, hidden_dim, 2, 2) # (8, 64, 64)\r\n",
    "        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim * 2, 2, 2) # (16, 32, 32)\r\n",
    "        self.conv3 = nn.Conv2d(hidden_dim * 2, hidden_dim * 4, 2, 2) # (32, 16, 16)\r\n",
    "        self.conv4 = nn.Conv2d(hidden_dim * 4, hidden_dim * 8, 2, 2) # (64, 8, 8)\r\n",
    "        self.conv5 = nn.Conv2d(hidden_dim * 8, hidden_dim * 8, 2, 1) # (64, 7, 7)\r\n",
    "        self.conv6 = nn.Conv2d(hidden_dim * 8, hidden_dim * 8, 2, 1) # (64, 6, 6)\r\n",
    "        self.conv7 = nn.Conv2d(hidden_dim * 8, hidden_dim * 8, 2, 1) # (64, 5, 5)\r\n",
    "        self.conv8 = nn.Conv2d(hidden_dim * 8, hidden_dim * 16, 1, 1) # (128, 5, 5)\r\n",
    "        self.conv9 = nn.Conv2d(hidden_dim * 16, hidden_dim * 16, 2, 1) # (128, 4, 4)\r\n",
    "        self.dropout1 = nn.Dropout(0.25)\r\n",
    "        self.fc1 = nn.Linear(2048, num_counts)\r\n",
    "        self.fc2 = nn.Linear(2048, num_particles)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.conv1(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv2(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv3(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv4(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv5(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv6(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv7(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv8(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv9(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.dropout1(x)\r\n",
    "        x = torch.flatten(x, 1)\r\n",
    "\r\n",
    "        y1 = self.fc1(x)\r\n",
    "        y1 = F.log_softmax(y1, dim=1)\r\n",
    "        y2 = self.fc2(x)\r\n",
    "        y2 = F.log_softmax(y2, dim=1)\r\n",
    "        return y1, y2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 18-layer Multi-output CNN (Late)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class MultiOutputCNN(nn.Module):\r\n",
    "    def __init__(self, num_particles=11, num_counts=4, hidden_dim=8):\r\n",
    "        super(MultiOutputCNN, self).__init__()\r\n",
    "        self.conv1 = nn.Conv2d(1, hidden_dim, 2, 2) # (8, 64, 64)\r\n",
    "        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim * 2, 2, 2) # (16, 32, 32)\r\n",
    "        self.conv3 = nn.Conv2d(hidden_dim * 2, hidden_dim * 4, 2, 2) # (32, 16, 16)\r\n",
    "        self.conv4 = nn.Conv2d(hidden_dim * 4, hidden_dim * 4, 2, 1) # (32, 15, 15)\r\n",
    "        self.conv5 = nn.Conv2d(hidden_dim * 4, hidden_dim * 4, 2, 1) # (32, 14, 14)\r\n",
    "        self.conv6 = nn.Conv2d(hidden_dim * 4, hidden_dim * 4, 2, 1) # (32, 13, 13)\r\n",
    "        self.conv7 = nn.Conv2d(hidden_dim * 4, hidden_dim * 4, 2, 1) # (32, 12, 12)\r\n",
    "        self.conv8 = nn.Conv2d(hidden_dim * 4, hidden_dim * 4, 2, 1) # (32, 11, 11)\r\n",
    "        self.conv9 = nn.Conv2d(hidden_dim * 4, hidden_dim * 4, 2, 1) # (32, 10, 10)\r\n",
    "        self.conv10 = nn.Conv2d(hidden_dim * 4, hidden_dim * 4, 2, 1) # (32, 9, 9)\r\n",
    "        self.conv11 = nn.Conv2d(hidden_dim * 4, hidden_dim * 8, 1, 1) # (64, 9, 9)\r\n",
    "        self.conv12 = nn.Conv2d(hidden_dim * 8, hidden_dim * 8, 2, 1) # (64, 8, 8)\r\n",
    "        self.conv13 = nn.Conv2d(hidden_dim * 8, hidden_dim * 8, 2, 1) # (64, 7, 7)\r\n",
    "        self.conv14 = nn.Conv2d(hidden_dim * 8, hidden_dim * 8, 2, 1) # (64, 6, 6)\r\n",
    "        self.conv15 = nn.Conv2d(hidden_dim * 8, hidden_dim * 8, 2, 1) # (64, 5, 5)\r\n",
    "        self.conv16 = nn.Conv2d(hidden_dim * 8, hidden_dim * 16, 1, 1) # (128, 5, 5)\r\n",
    "        self.conv17 = nn.Conv2d(hidden_dim * 16, hidden_dim * 16, 2, 1) # (128, 4, 4)\r\n",
    "        self.dropout1 = nn.Dropout(0.25)\r\n",
    "        self.fc1 = nn.Linear(2048, num_counts)\r\n",
    "        self.fc2 = nn.Linear(2048, num_particles)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.conv1(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv2(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv3(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv4(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv5(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv6(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv7(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv8(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv9(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv10(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv11(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv12(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv13(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv14(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv15(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv16(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv17(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.dropout1(x)\r\n",
    "        x = torch.flatten(x, 1)\r\n",
    "\r\n",
    "        y1 = self.fc1(x)\r\n",
    "        y1 = F.log_softmax(y1, dim=1)\r\n",
    "        y2 = self.fc2(x)\r\n",
    "        y2 = F.log_softmax(y2, dim=1)\r\n",
    "        return y1, y2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-output CNN (Early)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class MultiOutputCNN(nn.Module):\r\n",
    "    def __init__(self, num_particles=11, num_counts=4, hidden_dim=8):\r\n",
    "        super(MultiOutputCNN, self).__init__()\r\n",
    "        \r\n",
    "        \r\n",
    "        self.conv1 = nn.Conv2d(1, hidden_dim, 2, 2) # (8, 64, 64)\r\n",
    "        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim * 2, 2, 2) # (16, 32, 32)\r\n",
    "        \r\n",
    "        #Brached\r\n",
    "        self.conv3_b1 = nn.Conv2d(hidden_dim * 2, hidden_dim * 4, 2, 2) # (32, 16, 16)\r\n",
    "        self.conv4_b1 = nn.Conv2d(hidden_dim * 4, hidden_dim * 8, 2, 2) # (64, 8, 8)\r\n",
    "        self.conv5_b1 = nn.Conv2d(hidden_dim * 8, hidden_dim * 8, 2, 1) # (64, 7, 7)\r\n",
    "        self.conv6_b1 = nn.Conv2d(hidden_dim * 8, hidden_dim * 8, 2, 1) # (64, 6, 6)\r\n",
    "        self.conv7_b1 = nn.Conv2d(hidden_dim * 8, hidden_dim * 8, 2, 1) # (64, 5, 5)\r\n",
    "        self.conv8_b1 = nn.Conv2d(hidden_dim * 8, hidden_dim * 16, 1, 1) # (128, 5, 5)\r\n",
    "        self.conv9_b1 = nn.Conv2d(hidden_dim * 16, hidden_dim * 16, 2, 1) # (128, 4, 4)\r\n",
    "        self.dropout1_b1 = nn.Dropout(0.25)\r\n",
    "        self.fc1_b1 = nn.Linear(2048, num_counts)\r\n",
    "        \r\n",
    "        self.conv3_b2 = nn.Conv2d(hidden_dim * 2, hidden_dim * 4, 2, 2) # (32, 16, 16)\r\n",
    "        self.conv4_b2 = nn.Conv2d(hidden_dim * 4, hidden_dim * 8, 2, 2) # (64, 8, 8)\r\n",
    "        self.conv5_b2 = nn.Conv2d(hidden_dim * 8, hidden_dim * 8, 2, 1) # (64, 7, 7)\r\n",
    "        self.conv6_b2 = nn.Conv2d(hidden_dim * 8, hidden_dim * 8, 2, 1) # (64, 6, 6)\r\n",
    "        self.conv7_b2 = nn.Conv2d(hidden_dim * 8, hidden_dim * 8, 2, 1) # (64, 5, 5)\r\n",
    "        self.conv8_b2 = nn.Conv2d(hidden_dim * 8, hidden_dim * 16, 1, 1) # (128, 5, 5)\r\n",
    "        self.conv9_b2 = nn.Conv2d(hidden_dim * 16, hidden_dim * 16, 2, 1) # (128, 4, 4)\r\n",
    "        self.dropout1_b2 = nn.Dropout(0.25)\r\n",
    "        self.fc1_b2 = nn.Linear(2048, num_particles)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.conv1(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv2(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        \r\n",
    "        #Brached\r\n",
    "        y1 = self.conv3_b1(x)\r\n",
    "        y1 = F.relu(y1)\r\n",
    "        y1 = self.conv4_b1(y1)\r\n",
    "        y1 = F.relu(y1)\r\n",
    "        y1 = self.conv5_b1(y1)\r\n",
    "        y1 = F.relu(y1)\r\n",
    "        y1 = self.conv6_b1(y1)\r\n",
    "        y1 = F.relu(y1)\r\n",
    "        y1 = self.conv7_b1(y1)\r\n",
    "        y1 = F.relu(y1)\r\n",
    "        y1 = self.conv8_b1(y1)\r\n",
    "        y1 = F.relu(y1)\r\n",
    "        y1 = self.conv9_b1(y1)\r\n",
    "        y1 = F.relu(y1)\r\n",
    "        y1 = self.dropout1_b1(y1)\r\n",
    "        y1 = torch.flatten(y1, 1)\r\n",
    "        y1 = self.fc1_b1(y1)\r\n",
    "        y1 = F.log_softmax(y1, dim=1)\r\n",
    "\r\n",
    "        y2 = self.conv3_b2(x)\r\n",
    "        y2 = F.relu(y2)\r\n",
    "        y2 = self.conv4_b2(y2)\r\n",
    "        y2 = F.relu(y2)\r\n",
    "        y2 = self.conv5_b2(y2)\r\n",
    "        y2 = F.relu(y2)\r\n",
    "        y2 = self.conv6_b2(y2)\r\n",
    "        y2 = F.relu(y2)\r\n",
    "        y2 = self.conv7_b2(y2)\r\n",
    "        y2 = F.relu(y2)\r\n",
    "        y2 = self.conv8_b2(y2)\r\n",
    "        y2 = F.relu(y2)\r\n",
    "        y2 = self.conv9_b2(y2)\r\n",
    "        y2 = F.relu(y2)\r\n",
    "        y2 = self.dropout1_b2(y2)\r\n",
    "        y2 = torch.flatten(y2, 1)\r\n",
    "        y2 = self.fc1_b2(y2)\r\n",
    "        y2 = F.log_softmax(y2, dim=1)\r\n",
    "\r\n",
    "        return y1, y2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ResNet18"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class CustomResNet18Model(nn.Module):\n",
    "    def __init__(self, num_counts, num_particles):\n",
    "        super(CustomResNet18Model, self).__init__()\n",
    "        self.model_resnet = models.resnet18(pretrained=False)\n",
    "        self.model_resnet.conv1 = torch.nn.Conv1d(1, 64, (7, 7), (2, 2), (3, 3), bias=True)\n",
    "        \n",
    "        self.model_resnet.fc.register_forward_hook(lambda m, inp, out: F.dropout(out, p=0.5, training=m.training))\n",
    "        \n",
    "        num_ftrs = self.model_resnet.fc.in_features\n",
    "        self.model_resnet.fc = nn.Identity()\n",
    "        self.fc1 = nn.Linear(num_ftrs, num_counts)\n",
    "        self.fc2 = nn.Linear(num_ftrs, num_particles)\n",
    "    def forward(self, x):\n",
    "        x = self.model_resnet(x)\n",
    "        out1 = self.fc1(x)\n",
    "        y1 = F.log_softmax(out1, dim=1)\n",
    "        out2 = self.fc2(x)\n",
    "        y2 = F.log_softmax(out2, dim=1)\n",
    "        return y1, y2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## VGG16"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class CustomVgg16Model(nn.Module):\n",
    "    def __init__(self, num_counts, num_particles):\n",
    "        super(CustomVgg16Model, self).__init__()\n",
    "        self.model_vgg16 = models.vgg16(pretrained=False, progress=True)\n",
    "        self.model_vgg16.features[0] = torch.nn.Conv2d(1, 64, (3, 3), (1, 1), (1, 1))\n",
    "        num_ftrs = self.model_vgg16.classifier[0].in_features\n",
    "        self.model_vgg16.classifier = nn.Identity()\n",
    "        self.fc1 = nn.Linear(num_ftrs, num_counts)\n",
    "        self.fc2 = nn.Linear(num_ftrs, num_particles)\n",
    "    def forward(self, x):\n",
    "        x = self.model_vgg16(x)\n",
    "        out1 = self.fc1(x)\n",
    "        y1 = F.log_softmax(out1, dim=1)\n",
    "        out2 = self.fc2(x)\n",
    "        y2 = F.log_softmax(out2, dim=1)\n",
    "        return y1, y2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evalutate"
   ],
   "metadata": {
    "id": "9xNRaiMqZUt5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def evaluate(model, loss_fn, dataloader):\r\n",
    "    \"\"\"Evaluate the model on `num_steps` batches.\r\n",
    "    Args:\r\n",
    "        model: (torch.nn.Module) the neural network\r\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\r\n",
    "        dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches data\r\n",
    "    \"\"\"\r\n",
    "    model.eval()\r\n",
    "\r\n",
    "    accuracies = []\r\n",
    "    loss = 0.0\r\n",
    "    all_preds = []\r\n",
    "    all_labels = []\r\n",
    "    preds1 = []\r\n",
    "    preds2 = []\r\n",
    "    all_count_labels = []\r\n",
    "    all_particle_labels = []\r\n",
    "    all_images = []\r\n",
    "    for i, (inputs, count_labels, particle_labels) in enumerate(dataloader):\r\n",
    "        inputs = inputs.to(device)\r\n",
    "        if not args.multi_output:\r\n",
    "            outputs = model(inputs)\r\n",
    "            batch_loss = loss_fn(outputs, count_labels.squeeze(0).to(device))\r\n",
    "            loss += batch_loss\r\n",
    "            preds = torch.argmax(outputs, dim=-1)\r\n",
    "            all_preds.append(preds)\r\n",
    "            all_labels.append(count_labels)\r\n",
    "            loss = loss / len(dataloader)\r\n",
    "        else:\r\n",
    "            y1, y2 = model(inputs)\r\n",
    "            loss1 = loss_fn(y1, count_labels.squeeze(0).to(device)).detach().item()\r\n",
    "            loss2 = loss_fn(y2, particle_labels.squeeze(0).to(device)).detach().item()\r\n",
    "            batch_loss = 4 * loss1 + loss2\r\n",
    "            loss += batch_loss\r\n",
    "            y1 = torch.argmax(y1, dim=-1).to('cpu').numpy().tolist()\r\n",
    "            y2 = torch.argmax(y2, dim=-1).to('cpu').numpy().tolist()\r\n",
    "            \r\n",
    "            preds1.extend(y1)\r\n",
    "            preds2.extend(y2)\r\n",
    "            all_images.extend(inputs)\r\n",
    "            all_count_labels.extend(count_labels.to('cpu').numpy().tolist())\r\n",
    "            all_particle_labels.extend(particle_labels.to('cpu').numpy().tolist())\r\n",
    "    \r\n",
    "    torch.cuda.empty_cache()\r\n",
    "    \r\n",
    "    # Total accuracy\r\n",
    "    correct_pred = [1 if (preds1[i] == all_count_labels[i] and preds2[i] == all_particle_labels[i]) else 0 for i in range(len(preds1))]\r\n",
    "    accuracy = sum(correct_pred) / len(preds1) * 100\r\n",
    "    \r\n",
    "    # Count accuracy\r\n",
    "    correct_pred_count = [1 if (preds1[i] == all_count_labels[i]) else 0 for i in range(len(preds1))]\r\n",
    "    count_accuracy = sum(correct_pred_count) / len(preds1) * 100\r\n",
    "    \r\n",
    "    # Particle accuracy\r\n",
    "    correct_pred_particle = [1 if (preds2[i] == all_particle_labels[i]) else 0 for i in range(len(preds2))]\r\n",
    "    particle_accuracy = sum(correct_pred_particle) / len(preds1) * 100\r\n",
    "    \r\n",
    "    loss = loss / len(dataloader)\r\n",
    "    \r\n",
    "    # Compute accuracy for each particle type\r\n",
    "    particle_acc_dict = {}\r\n",
    "    for idx in idx2particle:\r\n",
    "        temp = {}\r\n",
    "        temp['crct'] = sum([1 if (preds2[i] == idx and preds2[i] == all_particle_labels[i]) else 0 for i in range(len(preds2))])\r\n",
    "        temp['total'] = sum([1 if (all_particle_labels[i] == idx) else 0 for i in range(len(all_particle_labels))])\r\n",
    "        particle_acc_dict[idx2particle[idx]] = temp['crct'] / temp['total']\r\n",
    "        if idx2particle[idx] == '7dx8':\r\n",
    "            wrong_pred_7dx8 = [preds2[i] if (all_particle_labels[i] == idx and preds2[i] != all_particle_labels[i]) else None for i in range(len(preds2))]\r\n",
    "    print(particle_acc_dict)\r\n",
    "\r\n",
    "    return accuracy, count_accuracy, particle_accuracy, loss"
   ],
   "outputs": [],
   "metadata": {
    "executionInfo": {
     "elapsed": 287,
     "status": "ok",
     "timestamp": 1621182654483,
     "user": {
      "displayName": "Enci Liu",
      "photoUrl": "",
      "userId": "13274077478020183171"
     },
     "user_tz": 420
    },
    "id": "90SAviZnS3vz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Draw confusion matrix for particle prediction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "_, _, test_dataloader = get_dataloaders(args, PARTICLES, PARTICLES, test_diff_particle=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "nb_classes = 11\r\n",
    "confusion_matrix = np.zeros((nb_classes, nb_classes))\r\n",
    "\r\n",
    "for i, (inputs, count_labels, particle_labels) in enumerate(test_dataloader):\r\n",
    "    inputs = inputs.to(device)\r\n",
    "    _, y2 = model(inputs)\r\n",
    "    y2 = torch.argmax(y2, dim=-1).to('cpu').numpy().tolist()\r\n",
    "    for t, p in zip(particle_labels, y2):\r\n",
    "        confusion_matrix[t, p] += 1\r\n",
    "\r\n",
    "plt.figure(figsize=(15,10))\r\n",
    "\r\n",
    "class_names = list(particle2idx.keys())\r\n",
    "df_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names).astype(int)\r\n",
    "heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\r\n",
    "\r\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right',fontsize=12)\r\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right',fontsize=12)\r\n",
    "plt.ylabel('True label', fontsize=15)\r\n",
    "plt.xlabel('Predicted label', fontsize=15)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "id": "1n0r3qXAtzEx"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train(args, model, optimizer, loss_fn):\r\n",
    "    \"\"\"\"\"\r\n",
    "    Train the network on the training data\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    EPOCH = args.epoches\r\n",
    "\r\n",
    "    train_dataloader, valid_dataloader, test_dataloader = get_dataloaders(args, \r\n",
    "                                                                          PARTICLES, \r\n",
    "                                                                          PARTICLES,\r\n",
    "                                                                          test_diff_particle=False)\r\n",
    "    step = 0\r\n",
    "\r\n",
    "    train_loss_values = []\r\n",
    "    train_accuracies = {\r\n",
    "        'Total': [],\r\n",
    "        'Count': [],\r\n",
    "        'Particle': []\r\n",
    "    }\r\n",
    "    valid_loss_values = []\r\n",
    "    valid_accuracies = {\r\n",
    "        'Total': [],\r\n",
    "        'Count': [],\r\n",
    "        'Particle': []\r\n",
    "    }\r\n",
    "    for epoch in range(EPOCH):\r\n",
    "        epoch_train_loss = 0.0\r\n",
    "        with tqdm(total=len(train_dataloader)) as t: \r\n",
    "            for i, (inputs, count_labels, particle_labels) in enumerate(train_dataloader):\r\n",
    "                step += 1\r\n",
    "                model.train()\r\n",
    "\r\n",
    "                inputs = inputs.to(device)\r\n",
    "                \r\n",
    "                if not args.multi_output:\r\n",
    "                    outputs = model(inputs)\r\n",
    "                    loss = loss_fn(outputs, count_labels.squeeze(0).to(device))\r\n",
    "                else:\r\n",
    "                    y1, y2 = model(inputs)\r\n",
    "                    loss1 = loss_fn(y1, count_labels.squeeze(0).to(device))\r\n",
    "                    loss2 = loss_fn(y2, particle_labels.squeeze(0).to(device))\r\n",
    "                    loss = 4 * loss1 + loss2\r\n",
    "                \r\n",
    "                optimizer.zero_grad()\r\n",
    "                loss.backward(retain_graph=True)\r\n",
    "                optimizer.step()\r\n",
    "\r\n",
    "                cost = loss.item()\r\n",
    "                \r\n",
    "                epoch_train_loss = cost\r\n",
    "                \r\n",
    "                t.set_postfix(train_loss='{:05.3f}'.format(cost))\r\n",
    "                t.update()\r\n",
    "                \r\n",
    "                # torch.cuda.empty_cache()\r\n",
    "        \r\n",
    "        if epoch % args.evaluate_every == 0:\r\n",
    "            valid_accuracy, valid_count_accuracy, valid_particle_accuracy, valid_loss = evaluate(model, criterion, valid_dataloader)\r\n",
    "            valid_accuracies['Total'].append(valid_accuracy)\r\n",
    "            valid_accuracies['Count'].append(valid_count_accuracy)\r\n",
    "            valid_accuracies['Particle'].append(valid_particle_accuracy)\r\n",
    "            valid_loss_values.append(valid_loss)\r\n",
    "            \r\n",
    "            train_accuracy, train_count_accuracy, train_particle_accuracy, train_loss = evaluate(model, criterion, train_dataloader)\r\n",
    "            train_accuracies['Total'].append(train_accuracy)\r\n",
    "            train_accuracies['Count'].append(train_count_accuracy)\r\n",
    "            train_accuracies['Particle'].append(train_particle_accuracy)\r\n",
    "            train_loss_values.append(train_loss)\r\n",
    "            \r\n",
    "            print(f'Step {step}: valid loss={valid_loss}, \\n valid accuracy={valid_accuracy}, \\n valid count accuracy={valid_count_accuracy}, \\n valid particle accuracy={valid_particle_accuracy}')\r\n",
    "            print(f'Step {step}: train loss={train_loss}, \\n train accuracy={train_accuracy}, \\n train count accuracy={train_count_accuracy}, \\n valid particle accuracy={train_particle_accuracy}')\r\n",
    "\r\n",
    "            \r\n",
    "    torch.save(model.state_dict(), args.ckpt_path)\r\n",
    "    \r\n",
    "    plot_loss(args, train_loss_values, 'train')\r\n",
    "    plot_accuracies(args, train_accuracies, 'train')\r\n",
    "    plot_loss(args, valid_loss_values, 'validation')\r\n",
    "    plot_accuracies(args, valid_accuracies, 'validation')\r\n",
    "    \r\n",
    "    test_accuracy, test_count_accuracy, test_particle_accuracy, _ = evaluate(model, criterion, test_dataloader)\r\n",
    "    print('Test accuracy: %f' % test_accuracy)\r\n",
    "    print('Test count accuracy: %f' % test_count_accuracy)\r\n",
    "    print('Test particle accuracy: %f' % test_particle_accuracy)"
   ],
   "outputs": [],
   "metadata": {
    "executionInfo": {
     "elapsed": 579,
     "status": "ok",
     "timestamp": 1621182655834,
     "user": {
      "displayName": "Enci Liu",
      "photoUrl": "",
      "userId": "13274077478020183171"
     },
     "user_tz": 420
    },
    "id": "18sO4iAot0bH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_accuracies(args, accuracies, split):\r\n",
    "    plt.figure(figsize=(6,5))\r\n",
    "    plt.title(f\"Total, count, and particle accuracies for {split} set\")\r\n",
    "    plt.plot(accuracies['Total'], label=\"Total\", color='r')\r\n",
    "    plt.plot(accuracies['Count'], label=\"Count\", color='g')\r\n",
    "    plt.plot(accuracies['Particle'], label=\"Particle\", color='b')\r\n",
    "    plt.xlabel(\"Epoch\")\r\n",
    "    plt.ylabel(\"Accuracy\")\r\n",
    "    plt.legend()\r\n",
    "    plt.savefig(f'{args.logdir}/{args.model}_{split}_accuracies_{dt.now()}.png')\r\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_loss(args, loss_values, split):\r\n",
    "    plt.figure(figsize=(6,5))\r\n",
    "    plt.title(f\"{split} loss\")\r\n",
    "    plt.plot(loss_values,label=\"train\", color='b')\r\n",
    "    plt.xlabel(\"Epoch\")\r\n",
    "    plt.ylabel(\"Loss\")\r\n",
    "    plt.legend()\r\n",
    "    plt.savefig(f'{args.logdir}/{args.model}_{split}_loss_{dt.now()}.png')\r\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run the pipeline"
   ],
   "metadata": {
    "id": "a8KR3TXppwp_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def load_model(args):\r\n",
    "    if args.model == 'multi_output_cnn':\r\n",
    "        num_particles = args.num_particles\r\n",
    "        num_counts = args.num_counts\r\n",
    "        hidden_dim = 8\r\n",
    "        model = MultiOutputCNN(num_particles=num_particles, num_counts=num_counts, hidden_dim=hidden_dim).to(device)\r\n",
    "    elif args.model == 'multi_output_resnet18':\r\n",
    "        num_particles = args.num_particles\r\n",
    "        num_counts = args.num_counts\r\n",
    "        model = CustomResNet18Model(num_counts, num_particles).to(device)\r\n",
    "    elif args.model == 'multi_output_vgg16':\r\n",
    "        num_particles = args.num_particles\r\n",
    "        num_counts = args.num_counts\r\n",
    "        model = CustomVgg16Model(num_counts, num_particles).to(device)\r\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "PARTICLES = ['1fpv', '1ss8', '3j03', '1ijg', '3iyf', '6ody', '6sp2', '6xs6', '7dwz', '7dx8', '7dx9']\r\n",
    "COUNTS = ['single', 'double', 'triple', 'quadruple']\r\n",
    "LENGTH = 1000"
   ],
   "outputs": [],
   "metadata": {
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1621182656734,
     "user": {
      "displayName": "Enci Liu",
      "photoUrl": "",
      "userId": "13274077478020183171"
     },
     "user_tz": 420
    },
    "id": "aY0fb-Sipdp2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "args = {\n",
    "    'model': 'multi_output_cnn', # multi_output_cnn || multi_output_resnet18 || multi_output_vgg16\n",
    "    'root_dir': './data/thumbnail',\n",
    "    'epoches': 20,\n",
    "    'batch_size': 128,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 1,\n",
    "    'num_particles': 11,\n",
    "    'num_counts': 4,\n",
    "    'length': 1000,\n",
    "    'evaluate_every': 1,\n",
    "    'logdir': './logs',\n",
    "    'multi_output': True\n",
    "}\n",
    "args = Namespace(**args)\n",
    "args.ckpt_path = f'{args.logdir}/{args.model}_checkpoint.pth'"
   ],
   "outputs": [],
   "metadata": {
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1621182690734,
     "user": {
      "displayName": "Enci Liu",
      "photoUrl": "",
      "userId": "13274077478020183171"
     },
     "user_tz": 420
    },
    "id": "dfjn88C4pYk4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "Define loss function and optimizer\n",
    "\n",
    "We will use the cross entropy loss and Adam optimizer\n",
    "\"\"\"\n",
    "\n",
    "# Create model\n",
    "model = load_model(args)\n",
    "\n",
    "# Define the cost function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer, learning rate \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)"
   ],
   "outputs": [],
   "metadata": {
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1621182693181,
     "user": {
      "displayName": "Enci Liu",
      "photoUrl": "",
      "userId": "13274077478020183171"
     },
     "user_tz": 420
    },
    "id": "HYHvT8q5-eKd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train(args, model, optimizer, criterion)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "summary(model, input_size=(1, 128, 128))"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 22717,
     "status": "ok",
     "timestamp": 1621182717152,
     "user": {
      "displayName": "Enci Liu",
      "photoUrl": "",
      "userId": "13274077478020183171"
     },
     "user_tz": 420
    },
    "id": "5PbSgdRfkpMG",
    "outputId": "b7978379-c6f9-4e28-b613-4c212493369e",
    "scrolled": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "pipeline.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "a8f61be024eba58adef938c9aa1e29e02cb3dece83a5348b1a2dafd16a070453"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}